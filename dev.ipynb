{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST, CelebA\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import id_gan\n",
    "from id_gan.vae import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cseweb.ucsd.edu/~weijian/static/datasets/celeba/img_align_celeba.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_transforms = transforms.Compose([ # Augmentation\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32), antialias=True),\n",
    "    transforms.Normalize(mean=(0.1307, ), std=(0.3081, ))\n",
    "])\n",
    "\n",
    "dataset = MNIST('./data', train=True, transform=image_transforms, download=True)\n",
    "# dataset = CelebA(\"./data\", transform=image_transforms, download=True)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "sample, _ = next(iter(data_loader))\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8ed8d9f4c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample[0, 0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "256 -> 128\n",
      "128 -> 64\n",
      "64 -> 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch 0] loss = 759.8875:   0%|          | 45/9380 [00:05<19:11,  8.10it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dmitry/src/hse-master/generative_models/final_project/dev.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dmitry/src/hse-master/generative_models/final_project/dev.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mid_gan\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dmitry/src/hse-master/generative_models/final_project/dev.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m id_gan\u001b[39m.\u001b[39;49mtrain_vae(\u001b[39m\"\u001b[39;49m\u001b[39mmnist\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/src/hse-master/generative_models/final_project/id_gan/vae.py:180\u001b[0m, in \u001b[0;36mtrain_vae\u001b[0;34m(config_name, batch_size, epochs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mnum_steps, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[epoch ?] loss = ?\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m    179\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mfor\u001b[39;00m inp_images \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m    181\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inp_images, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[1;32m    182\u001b[0m                 \u001b[39m# assume that the first item is image\u001b[39;00m\n\u001b[1;32m    183\u001b[0m                 inp_images \u001b[39m=\u001b[39m inp_images[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torchvision/transforms/transforms.py:708\u001b[0m, in \u001b[0;36mRandomHorizontalFlip.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[39m    img (PIL Image or Tensor): Image to be flipped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[39m    PIL Image or Tensor: Randomly flipped image.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp:\n\u001b[0;32m--> 708\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mhflip(img)\n\u001b[1;32m    709\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torchvision/transforms/functional.py:603\u001b[0m, in \u001b[0;36mhflip\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    601\u001b[0m     _log_api_usage_once(hflip)\n\u001b[1;32m    602\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 603\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mhflip(img)\n\u001b[1;32m    605\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mhflip(img)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:58\u001b[0m, in \u001b[0;36mhflip\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pil_image(img):\n\u001b[1;32m     56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be PIL Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mtranspose(_pil_constants\u001b[39m.\u001b[39;49mFLIP_LEFT_RIGHT)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/PIL/Image.py:2689\u001b[0m, in \u001b[0;36mImage.transpose\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2676\u001b[0m \u001b[39mTranspose image (flip or rotate in 90 degree steps)\u001b[39;00m\n\u001b[1;32m   2677\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2685\u001b[0m \u001b[39m:returns: Returns a flipped or rotated copy of this image.\u001b[39;00m\n\u001b[1;32m   2686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2688\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload()\n\u001b[0;32m-> 2689\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mtranspose(method))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/PIL/Image.py:536\u001b[0m, in \u001b[0;36mImage._new\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    534\u001b[0m new\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mmode\n\u001b[1;32m    535\u001b[0m new\u001b[39m.\u001b[39m_size \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39msize\n\u001b[0;32m--> 536\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mmode \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPA\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    537\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpalette:\n\u001b[1;32m    538\u001b[0m         new\u001b[39m.\u001b[39mpalette \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpalette\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import id_gan\n",
    "\n",
    "id_gan.train_vae(\"mnist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
